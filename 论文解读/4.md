标题：
LLM-empowered LearnerAgent: Simulating human-like learning dynamics in 12 months

### 优化后内容

#### 摘要
基于深度学习方法捕捉人类学习行为已成为心理学和智能系统领域的主要研究焦点。最近的方法依赖于受控实验或基于规则的模型来探索认知过程，但它们难以捕捉学习动态、跟踪随时间的进展或提供可解释性。为解决这些挑战，我们引入了LearnerAgent，这是一个基于大型语言模型（LLM）的新型多智能体框架，用于模拟真实的教学环境。为探索类人的学习动态，我们构建了具有心理基础特征的学习者，如深度型、表面型和懒惰型，以及无特征的通用学习者来检查基础LLM的默认行为。通过每周的知识获取、每月的策略选择、定期测试和同伴互动，我们可以跟踪个体学习者一整年的动态学习进展。我们的发现有四点：1）纵向分析显示，只有深度型学习者实现了持续的认知增长。我们特别设计的“陷阱问题”有效地诊断了表面型学习者的浅层知识。2）不同学习者的行为和认知模式与其心理特征密切一致。3）学习者的自我概念评分真实演变，通用学习者尽管有认知限制，但发展出令人惊讶的高自我效能感。4）关键的是，基础LLM的默认特征是“勤奋但脆弱的表面型学习者”——一种模仿好学生行为但缺乏真正可推广理解的智能体。大量模拟实验表明，LearnerAgent与真实场景高度吻合，产生了关于LLM行为的更有洞察力的发现。

#### 引言
随着大型语言模型（LLM）在现实世界应用中越来越普遍（Wei等人，2022；Guo等人，2025；Yuan等人，2025），研究人员开始研究模型行为与人类学习模式的相似之处和不同之处。这源于认识到研究这些相似性和差异可以帮助弥合人工和人类认知之间的差距（Park等人，2023；Tran等人，2025）。具体来说，教育心理学——具有关于人类学习过程的强大理论（Marton和Säljö，1976；Sweller，2011）——为深度学习系统提供了宝贵的见解。通过应用这些见解，研究人员可以探索广泛的学生学习行为（从表面学习到深度学习模式）。它能够有针对性地优化模型自身的学习机制，最终提高其在关键下游任务中的性能，包括自适应学习（Liu等人，2019）和模型能力诊断（Dong、Chen和Wu，2025）。

传统的交叉研究（Marton和Dahlgren，1976；Marsh，1990；Ezzaim等人，2023）主要依赖问卷调查或基于规则的测量模型来评估学生的学习成绩，无论是表面层面还是深层层面。然而，这种方法有明显的局限性：它不仅耗时费力，而且受限于从学习者的最终响应或模型输出中得出的静态推断。因此，它难以在复杂灵活的环境中对学习行为提供动态和可解释的评估。

受到LLM赋能的智能体在模拟复杂社会系统（如经济互动（Zhao等人，2024b）、自然科学（M. Bran等人，2024）和定量政治分析（Li、Gong和Jiang，2025））方面的成功启发，我们提出了一个新颖的LearnerAgent框架来解决上述挑战。如图1所示，LearnerAgent模拟了现实世界的教学环境。教师传授知识并布置任务，而学习者在12个月的旅程中进行学习和改进，完成考试并与同伴互动。

为了充分探索现实场景中的动态多样学习行为，我们从多个维度为学习者构建了不同的特征，如表1所示。学习者每周获取知识，并从知识巩固和认知反思中每月做出策略选择。定期测试全面评估他们的动态性能进展，而同伴互动和辩论模拟社会学习。

通过使用LearnerAgent进行的大量实验，我们发现了几个关键发现：
- LearnerAgent成功地在现实的教学环境中模拟了不同的学习者（深度型、表面型、懒惰型），每个学习者在学习策略、推理和认知努力方面都表现出与其特征一致的行为。
- 纵向分析表明，只有深度型学习者实现了可持续的认知增长。表面型学习者的知识证明是脆弱的，表现出捷径学习行为。
- 学习者的自我概念动态演变。有特征的学习者保持稳定的自我认知，而无特征的通用学习者发展出不断增加的自我效能感，反映了类人的信心增长。
- 学习者对同伴影响的反应不同：深度型学习者作为理性的辩论者，表面型学习者保持认知僵化，懒惰型学习者高度容易受到说服。
- 通用学习者（基础LLM）默认为“勤奋但脆弱的表面型学习者”风格，模仿理想学生的行为，同时缺乏更深的理解，这反映了对浅层模式匹配的依赖。

#### 相关工作
##### LLM赋能的智能体
由于LLM展示出的强大能力（Hurst等人，2024；Comanici等人，2025）和类人行为（Zhao等人，2025b），一些研究利用LLM的感知、规划、记忆和协作能力（Divband Soorati等人，2022；Fischer等人，2021）来模拟现实场景并构建更智能的智能体。通过不同智能体之间的协调，智能型AI可以分为三种类型：合作（Li等人，2023；Wang等人，2024）、竞争（Zhao等人，2024b；Liang等人，2024；Chen等人，2023）和竞合（Abdelnabi等人，2024；Davidson等人，2024）。利用和探索不同类型的协调，智能体已被广泛用于模拟现实世界的实施，包括经济市场（Zhao等人，2024b；Horton，2023；Li等人，2024）、人类行为（Aher、Arriaga和Kalai，2023；Dillion等人，2023；Park等人，2023）、社会科学（Li、Gong和Jiang，2025；Argyle等人，2023；Park等人，2022；Gürcan，2024；Qian等人，2023）和自然科学（M. Bran等人，2024；Boiko等人，2023）。

借鉴教育心理学的关键发现（Chin和Brown，2000；Ryan和Deci，2000），如从新手到专家的进步——其中表面型策略减少而深层型策略增加（认知发展）（Alexander，2004）；自我概念作为动机和行为的核心驱动因素的作用（Marsh，1990）；以及同伴影响对学术成绩的影响（DeLay等人，2016），我们模拟了一个现实的教学环境。这使我们能够更深入地探索人类认知发展和学习行为，为这些心理因素在互动环境中的表现提供新的视角。

##### LLM的捷径学习行为
由于训练方法、数据和模型架构的影响，LLM在训练过程中依赖捷径来学习某些特征之间的虚假相关性，导致推理过程中出现幻觉和错误（Weng，2024）。这种行为类似于人类表现出的捷径学习，即他们快速找到解决当前问题的捷径，而不深入研究 underlying 问题（Geirhos等人，2020；Zhao等人，2024a，2025a）。一些研究揭示了LLM中的捷径学习行为。Yuan等人（2024）发现LLM依赖不同的捷径，如词汇重叠、子序列、成分、否定、位置和风格，导致幻觉和下游任务失败。Tang等人（2023）指出，LLM表现得像懒惰的学习者，通过直接学习上下文示例中的潜在虚假相关性而放弃深入思考。

在本文中，我们将人类和模型学习进行了类比。通过多智能体系统模拟学习者，我们关注他们的学习发展，即深度学习、捷径学习和懒惰学习。

#### LearnerAgent框架
LearnerAgent通过角色扮演模拟构建了一个现实的学习环境，如图2所示，它基于教育心理学和认知科学的理论（Ryan和Deci，2000；Marsh和Martin，2011；DeLay等人，2016）模拟了全年的学习旅程。它包括两个主要角色：教师智能体和多个学习者智能体，每个都有独特的特征构建。

##### 特征构建
教师智能体（\(T\)）作为经验丰富的高中教师，负责传授知识和布置任务，包括组织教学活动、评估学习者表现和提供适当指导。

学习者智能体（\(L = \{L_d, L_s, L_l, L_g\}\)）分为四种预定义类型，其独特的行为模式源自个体角色特征。在全年（12个月）的学习中，所有学习者主要关注知识获取和定期评估。

为了模拟与现实场景一致的不同学习者，学习者根据三个维度以及基本信息被描绘为各种特征，如表1总结：1）学习动机：指学习的驱动力，包括内在好奇心、外部奖励（如成绩）和避免努力的愿望；2）初始自我概念评分：代表学习者对自己能力的评估，不仅反映在他们的信心水平上，还反映在他们受评估（如考试成绩）和同伴影响的程度上；3）发展策略：表示学习者在不同学习过程中采用的方法，与其学习动机和自我概念一致。

基于这些维度，我们将三种不同的学习者分类：深度型学习者（\(L_d\)）、表面型学习者（\(L_s\)）和懒惰型学习者（\(L_l\)）。此外，通用学习者（\(L_g\)）仅初始化为基本信息，作为比较的基本基线。值得注意的是，\(L_g\) 可以作为观察基础LLM自身出现的行为倾向的探针。这些学习者的详细配置可在附录A中找到。

##### 学习和改进
为了探索学习者的认知发展过程，通过一年的学习期精心设计了学习、改进和评估的结构化循环。具体来说，初始基线考试和循环结束时的最终考试设计用于评估整体知识获取和增长。在每个月中，学习者在前三周参与学习和练习，第四周用于总结、复习并进行月度考试。

- **每周学习和策略选择**：在每个月的前三周，学习者遵循结构化程序：学习提供的材料、做笔记并完成每周测试。每次测试后，\(T\) 提供所有问题的标准解释。在三周结束时，学习者需要做出两个反映自我调节的关键选择：1）知识巩固：将所有三周的学习笔记总结成一个文档，为月度考试做准备，或休息。2）认知反思：反思三周内的测试表现，识别关键见解和错误，或休息。
- **每月复习和评估**：每个月的第四周专注于最终复习和评估。在月度考试前，学习者应做出最终选择：彻底复习他们积累的材料（如总结、反思），或休息。
- **同伴互动和辩论**：为了模拟社会学习和同伴影响，LearnerAgent在每次月度考试后引入动态辩论机制。当两个学习者 \(L_i\) 和 \(L_j\) 对同一个问题 \(q\) 提供不同答案时，他们被提示进行辩论。在讨论期间，学习者可以为自己的推理辩护、挑战同伴或修改初始答案。教师组织辩论并确定其结构和轮数。辩论在以下任何条件满足时结束：1）学习者在被说服后明确改变答案；2）两个学习者都以有充分支持的推理维持自己的观点；3）他们的观点趋同或变得重复；4）辩论轮数已变得相对较高（超过 \(k_d\) 轮）。

##### 记忆机制
鉴于学习本质上是顺序性的，且LearnerAgent提供多样化的场景以促进学习者持续改进，我们采用短期和长期记忆机制来实现有效的知识获取、评估和反思。

- **短期记忆**通过在固定大小的队列中存储最近的 \(k\) 个对话轮次来维护即时对话上下文。这使得与教师和同伴进行连贯且上下文感知的互动成为可能。
- **长期记忆**作为学习者完整学习和改进历史的持久结构化存储。它存档关键记忆，包括知识总结、考试答案、自我反思、辩论记录、月度考试分数和自我概念分数，作为长期检索的不同条目。为了支持个性化帮助和时间对齐，我们引入了上下文相关的检索策略，其中框架根据当前时间步或学习阶段动态地为智能体提供仅最相关的记忆片段。如图2所示，该策略通过检索定制的记忆适应关键学习阶段：1）每周学习检索主题总结。2）月度考试提供先前的知识总结、反思、相关过去考试答案和教师反馈。3）辩论包括类似的过去响应和讨论线程。4）自我概念评估回忆先前的自我概念分数和表现比较。5）最终考试提供全年知识的整合。

##### 能力评估
为了全面评估学习者在学习过程中的知识和能力发展，我们采用了涵盖学术表现和心理变化的综合评估策略。

- **性能评估**：除了学习循环中的初始、每周、每月和最终考试外，构建陷阱问题以评估学习者超越死记硬背的更深层次理解。类似于（Yuan等人，2024），我们设计这些陷阱问题与每周练习问题结构相同，但由于微妙的上下文变化而推理不同，以深入诊断和揭示学习者的行为模式。我们使用强大的闭源LLM生成它们，然后手动注释和过滤，如图5所示。
- **心理评估**：为了更好地告知不同学习者的自我评估，为他们提供两个关键输入：1）他们的历史自我概念分数，2）其他学习者的考试分数。然后要求他们更新自己的自我概念分数（例如，在0-100分的量表上），考虑他们的学习能力、知识掌握、进展以及他们对自己相对于同学位置的看法。

#### 实验设置
在本节中，我们进行大量实验来评估LearnerAgent的有效性并回答以下研究问题（RQs）。
- **RQ1**：学习者在学习旅程中表现出哪些知识获取和认知发展过程？他们能解决陷阱问题吗？
- **RQ2**：每个学习者相关的不同行为和认知模式是什么？
- **RQ3**：学习者对自己的看法（自我概念）在学习过程中如何演变？
- **RQ4**：学习者在互动辩论中如何应对同伴影响？
- **RQ5**：基础LLM（作为无预定义特征的学习者——通用学习者）的新兴学习特征是什么？

#### 测试套件构建
我们将学习任务制定为英语语法获取，这是评估学习者知识掌握和深度推理的自然测试平台。为此，我们手动构建了一个新的测试套件来模拟一年的语法学习过程。所有材料均收集自权威的高考英语备考资源，包括高中课本、练习册和官方过去考试汇编，确保真实性和全面的语法覆盖。

测试套件包括两个关键组件：1）知识点：每周介绍特定的语法概念（如动词时态、从句类型、句子结构），与高考课程的顺序一致。2）测试问题：对于每个概念，我们提供多样化的问题格式——多项选择、填空和纠错——所有这些都旨在评估学习者灵活应用语法规则的能力。为了进一步评估超出死记硬背的深度理解，我们构建了陷阱问题：使用Gemini-2.5-Pro自动生成并手动验证。这些问题在结构或词汇上与过去学习的问题相似，但引入了微妙的上下文变化，要求学习者深入理解他们的知识。测试问题在以下定期测试中引入：
- 初始和最终考试：相同的100个问题集，旨在衡量学习期开始和结束时的整体语法水平。
- 每周练习：每周20个问题，以复习和巩固新获得的语法主题。
- 月度测试：共12次考试，每次包括50个问题，分布在三个部分：（1）15个复习问题，针对当月主题以评估短期保留；（2）15个陷阱问题以评估深度理解和迁移学习；（3）20个知识整合（K-I）问题整合所有涵盖的知识点以衡量累积熟练程度。

#### 实现细节
我们使用Qwen-2.5-72B-Instruct作为教师智能体提供指导和反馈，使用Qwen-2.5-7B-Instruct作为所有学习者智能体。每个学习者（深度型、表面型、懒惰型、通用型）配置有独特的特征，每个配置在3次独立运行中进行评估以确保鲁棒性。实验在四个NVIDIA A800-80GB GPU上进行。详细的提示设置可在附录A中找到，更多实验结果和分析可在附录B中找到。

#### 实验结果和分析
我们根据提出的四个研究问题，对不同学习者在各种场景中的学习和发展过程进行了一系列详细分析。

##### 认知和学习发展（RQ1）
我们通过分析三个方面来跟踪每个学习者的纵向发展：1）他们在12个月内的月度考试表现，2）他们在不同问题类型（如复习、陷阱和知识整合（K-I））上的表现以评估认知增益，3）他们从初始到最终考试的整体表现和推理能力。
