
《Shuffle-R1：多模态大语言模型高效强化学习框架》

### 1. 引言
受人类认知中缓慢而审慎的思维过程启发，推理能力使大型语言模型（LLM）能够进行规划、反思和超越单纯记忆的泛化。最近的进展表明，将强化学习（RL）整合到LLM的训练中可以显著增强这种能力，特别是在数学问题解决和代码生成等复杂领域。然而，当前的RL训练管道存在效率低下的问题，主要体现在优势坍缩（大多数优势集中在零附近）和滚动静默（贡献非零梯度的滚动比例随时间减少），这些问题导致次优的梯度更新并阻碍长期学习效率。

### 2. 相关工作
- **大推理模型**：早期研究通过对复杂的长思维链数据进行监督微调（SFT）在推理任务上取得性能提升，但有人认为SFT仅让模型记住推理步骤的格式和长思维链，未完全掌握独立推理能力。近期一些模型利用RL激发模型的独立探索能力，提升推理能力，而我们的工作聚焦于RL训练效率的深入研究并提出改进方案。
- **多模态大语言模型的强化学习**：一系列研究将RL应用于多模态大语言模型的训练和下游视觉任务，但多数工作主要关注RL在下游任务的适用性或高质量推理数据的组织等，而我们提出了一种引入动态自适应选择和重采样的新颖训练框架。

### 3. 预备知识
在LLM中的强化学习，策略梯度算法是常用方法，其目标函数是最大化模型从环境中获得的回报期望。但当前静态RL训练范式存在局限性，如大多数收集的滚动具有集中在零附近的优势，贡献非零梯度的滚动比例持续下降等，需要更高效的训练框架。

### 4. 方法
#### 4.1 问题分析
- **优势坍缩**：当前RL范式中大多数滚动的优势集中在零附近，导致梯度信号弱，简单增加滚动数量虽能在一定程度上提高性能，但会大幅增加计算开销，需要动态机制选择有价值的滚动。
- **滚动静默**：随着训练进行，贡献非零梯度的滚动比例持续下降，简单查询早期收敛，困难查询始终低精度，且标准训练管道中每个滚动仅使用一次，限制了对有价值滚动的充分利用，需要动态采样策略。

#### 4.2 成对轨迹采样（Pairwise Trajectory Sampling，PTS）
为缓解优势坍缩，提出PTS模块，将候选滚动组织成结构化的对比对，仅保留具有最大优势对比的对用于训练。给定查询和滚动大小，将滚动轨迹分组排序，构造配对集，通过简单的top - k采样策略选择有效对，使梯度更新聚焦于多样化和梯度丰富的轨迹。

#### 4.3 基于优势的批量洗牌（Advantage - based Batch Shuffle，ABS）
为克服滚动静默问题，提出ABS模块，基于绝对优势之和为每个轨迹对分配重要性权重，根据采样概率对子采样，重新构造训练批量，增加高优势轨迹的更新频率，强化高价值样本的暴露。

### 5. 实验
#### 5.1 实验设置
- **数据集和基准**：在Geometry3K、MMK12等数据集上进行实验，评估模型在域内和域外视觉推理基准（如MathVerse、MathVision等）上的性能。
- **实现细节**：使用EasyR1作为训练代码库，选择Qwen2.5 - VL - 3B - Instruct和Qwen2.5 - VL - 7B - Instruct作为基础模型，设置相关超参数，进行评估。

#### 5.2 主要结果
- **与代表性算法比较**：在Geometry3K和K12数据集上与GRPO和DAPO比较，我们的方法在域内和域外基准上均取得性能提升，如在Geometry3K数据集上3B模型精度从GRPO的42.64%提升到47.88%。
- **与基于RL的模型比较**：在MM - Eureka数据集上进行大规模实验，我们的模型在多个基准上表现优异，超过一系列开源7B竞争模型，甚至与一些闭源模型相比也具有竞争力。
- **效率分析**：图2（a）显示在相同更新滚动大小下，我们框架训练的模型比GRPO表现更好，体现了更好的训练数据利用效率；优势分布分析表明PTS有效缓解了优势坍缩；训练动态和验证精度等分析表明我们的框架在更少训练步骤下达到可比性能，且缓解了滚动静默问题。

#### 5.3 消融研究
- **算法设计的有效性**：逐步引入PTS和ABS模块，结果表明两者联合应用能显著提升模型性能，如在Geometry3K测试集上，PTS使模型精度从42.64%提升到46.21%，加入ABS后进一步提升到47.88%。
- **成对轨迹采样的合理性**：通过设计对比实验，验证了结构化对比采样方案的有效性，单向正负采样和无偏随机采样性能不如PTS。
- **基于优势的批量洗牌的合理性**：对比实验表明ABS能显著提升性能，无偏洗牌和静态重排设置性能不如ABS。
- **超参数**：研究了PTS中的采样比和ABS中的洗牌次数等超参数对性能的影响，选择合适的超参数以平衡信号质量和计算效率。

### 6. 结论
提出Shuffle - R1框架，通过成对轨迹采样和基于优势的批量洗牌，在域内和域外任务上显著优于代表性算法和模型，展示了以数据为中心的自适应设计的价值。

开源地址：https://github.com/XenoZLH/Shuffle-R1