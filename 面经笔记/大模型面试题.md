# 2024国内24家大模型面试经验（精简版）

## 前言
2023年3月前后国内大模型爆发，我陆续面试了多家大厂和初创公司。近期很多朋友咨询相关面试，遂整理面经供同行交流。因最早面试距今已半年，细节难免模糊，列表按面试时间排序，后文附个人经验总结与核心考点，仅供参考。


## 24家公司面试详情
1. **智元机器人（Agibot）- 拒**  
   稚晖君团队，早期面试（团队仍在组建）。一面由稚晖君主持，问简历经历+Transformer基础（MHA、复杂度、BERT/GPT），无后续反馈。个人看好其具身智能方向，与大佬交流顺畅。

2. **面壁智能 - offer**  
   共2面：技术面问大模型训练、Transformer+过往经历；二面聊offer细节（记忆中无额外技术题）。团队多为清华背景，较年轻，资金充足，薪资待遇可观。

3. **光年之外 - 简历拒**  
   2023年3月由猎头推荐，简历初筛未通过。

4. **北京智源研究院 - 流程中断**  
   共3-4面，面试官来自evaluation、训练等团队，倾向招evaluation方向人才。某轮面试官爽约后，承诺重新安排但无后续，默认为拒。

5. **360 - 简历拒**  
   猎头反馈岗位要求P8+，简历未达标。

6. **Minimax - 口头offer（未接受）**  
   至少4轮面试：前几轮以LeetCode为主，大模型问题少且浅（含1轮手写MHA）；终面后分配至框架组，咨询算法组岗位被拒，未继续接触。薪资总包高（初创中靠前），但面试官未深入沟通公司前景、技术方向，主管面仅20分钟。

7. **昆仑万维 - offer**  
   面试问大模型训练细节（参数规模、训练流程），无LeetCode。面试官态度亲和，个人拿到的薪资偏低，但后续猎头称其常规待遇较好，可能是个人沟通或记忆偏差。

8. **云从科技 - 拒**  
   共2面：二面原定1小时，实际聊2小时+，深入问过往经验、大模型训练（模型端/框架端）、研究内容，沟通愉快但未通过。

9. **阿里夸克 - offer**  
   共4面：2轮含LeetCode+浅问经验；三面问大模型核心（Transformer、训练、分布式、loss spike处理）；四面为cross面（推荐组大佬），问搜广推、概率题、脑筋急转弯+OS题（个人不会）。后续收到offer，但与HR沟通时存在摩擦，对方态度欠礼貌。

10. **衔远 - offer（未接受）**  
    共3面，问题中规中矩，终面与周伯文老师沟通。岗位为框架方向，个人更倾向模型岗，虽薪资高但未接受。

11. **潞晨科技（Colossal-AI）- 拒**  
    主打框架，一面深问框架内容（模型切分方式、Flash-Attention），个人回答尚可；后表达想做算法岗（其算法组HC少），二面算法组问应用端问题（推测不做基座模型），未通过。行业内框架端口碑好，曾见公司相关新闻。

12. **蚂蚁（徐鹏老师团队）- offer**  
    共3面：一面细节记不清；二面徐鹏老师亲面，聊过往经历（大模型、搜广推、本科前后端实习），强调团队“研工不分家”（模型研究+工程都做），与个人理念契合；HR面后发offer，首年含签字费。

13. **腾讯 - 简历拒**  
    猎头反馈岗位要求博士学历，简历未通过。

14. **小红书 - 简历拒**  
    同腾讯，岗位要求博士，简历未通过。

15. **商汤 - 拒**  
    共2面：一面LeetCode+手写MHA，自我感觉良好；二面表现不佳（具体问题记不清），无后续反馈。

16. **百川智能 - 拒**  
    猎头推荐，一面通过后，二面因个人记错时间（当时在外开车）迟到，HR以“需3年以上经验”为由拒绝（推测为委婉说法）。

17. **百度文心 - offer**  
    多轮面试：一面为数据组，问大模型数据处理（回答一般，表达想转模型组后，后续由模型组面试官对接）；无明确LeetCode环节，问题以Transformer、大模型训练为主；与leader、HR沟通顺畅。部门分工细（模型/框架/数据分开），底薪较高（具体月数记不清）。

18. **科大讯飞 - 拒**  
    HR初沟通时，认为个人大概率不愿去合肥，直接拒绝推进。

19. **IDEA研究院 - 拒**  
    仅1面即未通过，具体面试内容记不清。

20. **好未来 - offer（未接受）**  
    团队背景在同行中偏一般：一面LeetCode简单题未答出（尴尬），但模型相关面试表现较好，最终发offer。岗位做数学相关GPT模型（个人不熟悉方向），未沟通薪资细节。

21. **零一万物 - 拒**  
    共2面：一面（阿里推荐系统大佬）问LeetCode+推荐系统，回答尚可；二面（搜广推/NLP经验丰富的大佬）考较难LeetCode，未答出，未进入后续环节。好奇后续是否有大模型方向面试官参与。

22. **月之暗面（Moonshot）- 拒**  
    国内面试中技术最专业、相关性最高的一家：一面面试官为Kaiming高引论文合作者，问大模型训练基础、Transformer架构+框架端内容（分布式训练切分、设备间通信），follow-up问底层通信算法（如ring-reduce原理，个人不会）；还问CUDA使用经验（个人仅浅层了解），加试难LeetCode（记不清题目）未写出。公司创始人及员工背景顶尖，虽仅1面未过，但印象深刻。

23. **阿里达摩院（新达摩）- offer**  
    共3面：一面LeetCode+浅问大模型技术；二面为团队大老板（海归连续创业者，汇报至行癫），聊大模型技术愿景、市场方向（个人部分内容未听懂，但收获大）；因一面LeetCode表现一般，补1轮coding面后发offer。团队做纯研究（大语言模型+多模态），算力充足。

24. **边塞科技 - 拒**  
    专注RLHF与微调，团队为清华背景，创始人有OpenAI经验。共2面，深入聊RLHF（个人学到很多），但个人RL水平仅“票友级”，未通过。


## 个人面试总结
1. **行业现状**：大模型领域竞争激烈，新模型、新论文迭代速度快于学习速度。  
2. **能力要求**：研究岗需懂工程，工程岗需懂模型；尤其初创公司的硬核岗位，常要求覆盖“应用+模型+框架+底层后端+硬件”多方向。  
3. **个人技术侧重**：主做基座预训练算法，懂基础框架/RL（面试够用），但ML compiler、kernel、CUDA等底层知识浅，硬件完全不懂；后续计划在不放弃模型的前提下，补底层能力（如kernel）。  
4. **其他感悟**：市场前景尚不清晰（个人为技术宅，此前不关注市场）；RLHF是潜力方向，RL经验需深挖；同职级下，大模型岗位薪资普遍偏高；多数公司聚焦语言模型，偏研究的团队会涉及多模态预训练。


## 高频考点（按重要性排序，针对基座算法/框架岗）
1. **多头注意力（MHA）**：coding/概念必考点，需掌握时间/空间复杂度、优化手段（KV-Cache、MQA、GQA）、手写代码。  
2. **归一化（Norm）**：考概念（作用原理）或手写代码，频率高且内容标准。  
3. **框架相关**：各类并行方式（优缺点）、DeepSpeed/Megatron源码理解、Flash-Attention，常考代码题。  
4. **主流大模型细节**：BERT/GPT的位置编码、训练loss、激活函数、架构差异，自回归机制是重点。  
5. **大模型训练经验**：针对有工作经验者，问训练问题处理（如loss炸掉的解决方案）、实操技巧，面试官会通过细节确认是否真做过基座训练。  
6. **数据预处理**：BPE、tokenization、mask机制（对模型/训练的影响）、数据配比（参考相关论文）。  
7. **模型评估（evaluation）**：评估维度（安全性、有效性）、公开数据集，少数考手写eval框架（多选/生成任务）。  
8. **选考内容**：多模态、RLHF（偏研究岗，需看相关论文），根据投递岗位调整复习重点。


## 面试题精选（精简答案）
1. **BERT原理与应用**：基于双向Transformer编码器的预训练模型，通过MLM（掩码语言模型）和NSP（下一句预测）预训练，应用于文本分类、命名实体识别等NLP任务。  
2. **Seq2Seq模型**：将一个序列映射到另一个序列的模型（如“编码器+解码器”结构），应用于机器翻译、对话生成。  
3. **Transformer原理与优势**：基于自注意力机制的序列模型，优势是并行计算（比RNN快）、捕捉长距离依赖能力强，适用于NLP/CV等领域。  
4. **注意力机制**：给输入不同部分分配不同权重，提升长序列处理能力，应用于机器翻译、文本摘要。  
5. **CNN在计算机视觉的应用与优势**：通过卷积层/池化层提取图像特征，应用于图像分类、目标检测；优势是参数共享、平移不变性。  
6. **GAN原理与应用**：含生成器（生成假数据）和判别器（区分真假），对抗训练至平衡，应用于图像生成、修复。  
7. **强化学习（RL）原理与应用**：智能体通过与环境交互（试错）学习最优策略，应用于游戏、机器人控制。  
8. **自监督学习**：无需人工标注，模型自动生成标签训练（如BERT的MLM），优势是解决数据标注成本高的问题。  
9. **迁移学习**：将A任务的知识迁移到B任务，适用于B任务数据稀缺的场景（如用ImageNet预训练模型做小数据集分类）。  
10. **模型蒸馏**：用大模型（教师）指导小模型（学生）训练，减少小模型的计算/存储开销，适用于移动端部署。  
11. **LSTM原理与应用**：带门控单元（输入门/遗忘门/输出门）的RNN，解决长距离依赖问题，应用于语言建模、时间序列预测。  
12. **BERT的MLM任务**：预训练时随机掩码输入文本中的部分词汇，让模型预测掩码内容，目的是让模型学习双向上下文语义。